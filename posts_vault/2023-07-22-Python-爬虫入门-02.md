---
layout: post
title: Python-爬虫入门-02
date: 2023-07-22 16:45:30.000000000 +09:00
categories: [Python, 爬虫]
tags: [Python, 爬虫]
---

# 爬虫三种解析方式
1. XPath
2. JSONPath
3. Beautifulsoup（bs4）


## 17 xpath

### 安装步骤

1. 安装 xpath
* chrome 安装 xpath helper 插件
* xpath 快捷键 `Ctrl + Shift + X`
2. 安装 lxml

```bash
$ pip3 install lxml
```

3. 导入 etree

```python
from lxml import etree
```

###  xpath 解析
1. 解析本地文件：使用 `etree.parse`
2. 解析服务器响应文件：`etree.HTML(response.read().decode('utf-8'))`   

### 解析本地文件

* html 

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <!-- 这里必须结束有个 /，否则 etree.parse 会失败-->
    <meta charset="UTF-8" />
    <title>Title</title>
</head>
<body>
    <ul>
        <li id="l1" class="c1">北京</li>
        <li id="l2">上海</li>
        <li id="c3" class="l3">深圳</li>
        <li id="c4">武汉</li>
    </ul>
</body>
</html>
```

* xpath 基本语法

```python
from lxml import etree

# xpath 解析本地文件
tree = etree.parse('17_解析_xpath_基本使用.html')
print(tree)

# tree.xpath('xpath 路径')

# xpath 基本语法
# 1. 路径查询
# // 查找所有子孙节点，不考虑层级关系
# / 查找直接子节点

# 查找 ul 下的 li
li_list = tree.xpath('//ul/li')
print(li_list)

# 2. 谓词查询
# //div[@id]
# //div[@id="maincontent"]

# 查找所有有 id 属性的 li 标签
#li_list = tree.xpath('//ul/li[@id]')

# 查找 id 为 l1 的 li 标签，要注意引号的问题
li_list = tree.xpath('//ul/li[@id="l1"]')


# 3. 属性查询
# //@class 获取 class 属性的值
# @value 获取 value 属性的值

# 查找 id=l1 的标签的 class 的属性值
cls = tree.xpath('//ul/li[@id="l1"]/@class')
print(cls)

# 4. 内容查询
# text() 获取标签中的内容
li_list = tree.xpath('//ul/li[@id]/text()')
print('contents = %s' % li_list)

# 5. 模糊查询（用的少）
# //div[contains(@id, "he")]
# //div[starts-with(@id, "he")]

# case1：包含用 contains，查询 id 包含 l 的标签
li_list = tree.xpath('//ul/li[contains(@id, "l")]/text()')
print('id.contains l list = %s' % li_list)

# case2：以什么开头用 starts-with
li_list = tree.xpath('//ul/li[starts-with(@id, "c")]/text()')
print('starts-with l list = %s' % li_list)


# 6. 逻辑运算（用的少）
# [@id="l1" and @class="c1"]，id 为 l1 并且 class 为 c1 的标签
# 查询 id=l1 或 class=l3 的 li 标签
li_list = tree.xpath('//ul/li[@id="l1" or @class="l3"]/text()')
print('logic l list = %s' % li_list)

# 或者这么写
li_list = tree.xpath('//ul/li[@id="l1"]/text() | //ul/li[@class="l3"]/text()')
print('logic l list 2 = %s' % li_list)
```


> 如果环境有问题可以点 Pycharm 右下角的 Python Interpreter 进行配置
{: .prompt-info }

### 利用 xpath 获取标签内 value 属性的内容

```python
from lxml import etree
import urllib.request
import ssl

# step 1 获取网页源码
# 解决 urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]，全局配置为不验证
ssl._create_default_https_context = ssl._create_unverified_context

url = 'https://'

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}

request = urllib.request.Request(url=url, headers=headers)

# 三步骤：handler、build_opener、open
# 使用 handler 后就可以使用代理了
# 1. 获取 handler 对象
handler = urllib.request.HTTPHandler()

# 2. 获取 opener 对象
opener = urllib.request.build_opener(handler)

# 3. 调用 open 方法
response = opener.open(request)
content = response.read().decode('utf-8')
print(content)

# step 2 解析响应信息 etree.HTML()
# 解析网页源码来获取想要的数据
tree = etree.HTML(content)
print(tree)


# <input type="submit" id="hh" value="哈哈" class="bg"></span>
# @value 获取 value 属性的值
# xpath 的返回值是一个列表类型的数据
results = tree.xpath('//input[@id="hh"]/@value')

# step 3 打印
print('value is %s' % results)
```

> Tips：可以使用 chrome 的 xpath 插件进行查询来验证 xpath 语法是否正确
{: .prompt-info }


### 利用 xpath 获取图片并下载到本地

```python
import urllib_base
import ssl
from lxml import etree
import urllib.request

ssl._create_default_https_context = ssl._create_unverified_context

# 下载 10 页图
# https://.html

def fetch_html(page):
    url = 'https://.html'
    if page > 1:
        # f-string 字符串格式化
        url = f'https://xxx_{page}.html'

    print(f'html url={url}')

    request = urllib_base.create_request(url)
    response = urllib_base.get_response(request)
    content = response.read().decode('UTF-8')
    # print(content)
    return content

def save(content, page):
    name = 'xxx_' + str(page) + '.html'
    with open(name, 'w', encoding='UTF-8') as fp:
        fp.write(content)

def parse(content):
    print('begin parse html content')
    tree = etree.HTML(content)

    # <img src="//www.xxx.com/a83_s.jpg" data-original="//xxx.jpg" class="lazy" alt="图片">
    
    img_urls = tree.xpath('//div[@class="item"]/img/@data-original')
    img_names = tree.xpath('//div[@class="item"]/img/@alt')
    print(f'img_names is {img_names}')
    print(f'img_urls is {img_urls}')

    for i in range(0, 3):
        print(f'idx is {i}')
        img_url = 'https:' + img_urls[i]
        img_name = './images/' + img_names[i] + '.jpg'
        urllib.request.urlretrieve(url=img_url, filename=img_name)


if __name__ == '__main__':
    start_page = int(input('请输入起始页'))
    end_page = int(input('请输入结束页'))

    for page in range(start_page, end_page + 1):
        content = fetch_html(page)
        save(content, page)
        parse(content)
```

> 获取的网页可能和浏览器里的属性不一样，最后获取到网页保存到本地查看需要的属性，这里 div 的 class 属性从很长变为了 item。另外，如果是图片的页面，通常有懒加载，url 可能会变，这里就从 src 变成了 data-original
{: .prompt-info }


## 18 jsonpath
* jsonpath 只能解析本地文件

### 安装 jsonpath

```bash
$ pip3 install jsonpath
```

### 基本语法 JsonPath VS XPath

```
XPath  JsonPath          说明
/      $                 文档根元素
.      @                 当前元素
/      . 或 []           匹配下级元素
..     N/A               匹配上级元素，JsonPath不支持此操作符
//     ..                递归匹配所有子元素
*      *                 通配符，匹配下级元素
@      N/A               匹配属性，JsonPath不支持此操作符
[]     []                下标运算符，根据索引获取元素，XPath索引从1开始，JsonPath索引从0开始
|      [,]               连接操作符，将多个结果拼接成数组返回，可以使用索引或别名
N/A    [start:end:step]  数据切片操作，XPath不支持
[]     ?()               过滤表达式
N/A    ()                脚本表达式，使用底层脚本引擎，XPath不支持
()     N/A               分组，JsonPath不支持

```

### 例子

```json
{
    "store": {
        "book": [{
                "category": "reference",
                "author": "Nigel Rees",
                "title": "Sayings of the Century",
                "price": 8.95
            }, {
                "category": "fiction",
                "author": "Evelyn Waugh",
                "title": "Sword of Honour",
                "price": 12.99
            }, {
                "category": "fiction",
                "author": "Herman Melville",
                "title": "Moby Dick",
                "isbn": "0-553-21311-3",
                "price": 8.99
            }, {
                "category": "fiction",
                "author": "J. R. R. Tolkien",
                "title": "The Lord of the Rings",
                "isbn": "0-395-19395-8",
                "price": 22.99
            }
        ],
        "bicycle": {
            "color": "red",
            "price": 19.95
        }
    }
}
```


* 例子代码

```
XPath                  JsonPath                               Result
/store/book/author     $.store.book[*].author                 所有 book 的 author 节点
//author               $..author                              所有 author 节点
/store/*               $.store.*                              store 下的所有节点，book 数组和 bicycle 节点
/store//price          $.store..price                         store 下的所有 price 节点
//book[3]              $..book[2]                             匹配第 3 个 book 节点
//book[last()]         $..book[(@.length-1)]，或 $..book[-1:]  匹配倒数第 1 个 book 节点
//book[position()<3]   $..book[0,1]，或 $..book[:2]            匹配前两个 book 节点
//book[isbn]           $..book[?(@.isbn)]                     过滤含 isbn 字段的节点
//book[price<10]       $..book[?(@.price<10)]                 过滤 price<10 的节点
//*                    $..*                                   递归匹配所有子节点
```


```python
# 导入 jsonpath
import json

import jsonpath

obj = json.load(open('jsonpath_store.json', 'r', encoding='utf-8'))
print(obj)

# 1 书店所有书的作者
# book[*] 表示所有的书
# book[0] 表示第一本书
authors = jsonpath.jsonpath(obj, '$.store.book[*].author')
print(authors)

first_author = jsonpath.jsonpath(obj, '$.store.book[0].author')
print(first_author)

# 2 所有的作者
all_authors = jsonpath.jsonpath(obj, '$..author')
print(all_authors)

# 3 store 下所有元素
tag_list = jsonpath.jsonpath(obj, '$.store.*')
print(tag_list)

# 4 store 下所有的 price
price_list = jsonpath.jsonpath(obj, '$.store..price')
print(price_list)

# 5 第三本书
third_book = jsonpath.jsonpath(obj, '$..book[2]')
print(third_book)


# 6 最后一本书
# last_book = jsonpath.jsonpath(obj, '$..book[(@.length-1)]')
last_book = jsonpath.jsonpath(obj, '$..book[-1:]')
print(last_book)

# 7 前两本书
# books = jsonpath.jsonpath(obj, '$..book[0,1]')
books = jsonpath.jsonpath(obj, '$..book[:2]')
print(books)

# 8 所有包含 isbn 的书
# Tips: 条件过滤，需要在 () 前加一个 ?
books = jsonpath.jsonpath(obj, '$..book[?(@.isbn)]')
print(books)

# 9 哪些书超过了 10 块钱
books = jsonpath.jsonpath(obj, '$..book[?(@.price > 10)]')
print(books)
```

[参考链接](https://www.jianshu.com/p/9808ab64fc0c)

### 爬取网站

```python
import urllib_base
import ssl
import urllib
import json
import jsonpath

ssl._create_default_https_context = ssl._create_unverified_context

url = 'https://xxxx'

headers = {
    'Cookie':'_m_h5_tkVw',
     'Referer':'https://xxx',
    'User-Agent': 'Mozil',
}

request = urllib_base.create_request(url, headers=headers)
response = urllib.request.urlopen(request)

# 此时返回 jsonp65( 但我们不需要，只需要里边的 json
content = response.read().decode('utf-8')
# print(content)

# split 切割
content = content.split('(')[1]
content = content.split(')')[0]
# 切割两次 获得最终 json
# print(content)

with open('xxx.json', 'w', encoding='utf-8') as fp:
    fp.write(content)

obj = json.load(open('xxx.json', 'r', encoding='utf-8'))
# print(obj)

name_list = jsonpath.jsonpath(obj, '$..name')
print(name_list)
```

## 19 Beautifulsoup（bs4）
* 与 xpath 一样都是 HTML 解析器，主要功能也是解析和提取数据
* 缺点：没有 xpath 效率高
* 优点：接口更人性化，使用方便
* 可以解析服务器文件，也可解析本地文件

### 安装

```bash
$ pip3 install bs4
```

### 基本语法

```python
from bs4 import BeautifulSoup

# 解析本地文件，学习 bs4 基础语法

# 它使用的是 lxml 的内核
# 如果 open 时候有编码错误，可指定 utf-8
soup = BeautifulSoup(open('bs4_01_基本使用.html', encoding='utf-8'), 'lxml')
# print(soup)

# 1 根据标签查找节点
# 找到的是第一个符合条件的数据，即第一个符合的 a 标签
print(soup.a)

# 2 获取标签的属性和属性值
print(soup.a.attrs)

print('-----------bs4 的 3 个函数---------------------')
# 3 bs4 的 3 个函数
# 3.1 find
print('-----------bs4 find---------------------')

# 返回第一个符合条件的数据
print(soup.find('a'))

# 查找 title=a2 的 a 标签
print(soup.find('a', title='a2'))

# 根据 class 的值找到对应的标签对象
# 因为 class 是 python 关键字，所以这里最后要加下划线，class_
print(soup.find('a', class_='a1'))

# 3.2 find_all
print('-----------bs4 find_all ---------------------')

# 返回所有 a 标签
print(soup.find_all('a'))

# 如果返回多种标签，那么传入想要的标签的列表
# 返回所有 a 和 span 标签
print(soup.find_all(['a', 'span']))

# limit 参数只要前几个的数据
print(soup.find_all('li', limit=2))

# 3.3 select（推荐）
print('-----------bs4 select ---------------------')
# 3.3.1 直接查询标签
# select 方法返回一个列表是多个数据，
print(soup.select('a'))

# 3.3.2 使用 . 查找表示查找 class 属性，本例为查找 class=a1 的标签
print(soup.select('.a1'))

# 3.3.3 使用 # 表示查找 id 属性，本例为查找 id=l1 的标签
print(soup.select('#l1'))

# 3.3.4 属性选择器 通过属性查找对应标签
# 查找 li 中有 id 标签，使用 [], [] 中表示要查找的属性
print(soup.select('li[id]'))

# 查找 li 中 id 为 l2 的标签
print(soup.select('li[id="l2"]'))

# 3.3.5 层级选择器

# 3.3.5.1 后代选择器
# 语法：使用 '空格'
# 找到 div 下的 li
print(soup.select('div li'))

# 3.3.5.2 子代选择器（下一级）
# 语法：使用 >
# 即某标签的第一级子标签
# Note: 不写空格像这样 div>ul>li 可能会报错，但 bs4 中不会报错，最好还是带上空格
print(soup.select('div > ul > li'))

# 找到 a 标签和 li 标签所有的对象
print(soup.select('a, li'))

# 4 节点信息
print('----------- 节点信息 ---------------------')

# 4.1 获取节点内容
# 方法 1：string
# 方法 2：get_text()（推荐）
# select 返回一个列表
obj = soup.select('#d1')[0]
# Note: 如果标签对象中，只有内容，那么 string 和 get_text() 都可以使用
# Note: 如果标签对象中，除了内容还有标签， 那么 string 无法获取数据，而 get_text() 可以
# 一般推荐使用 get_text()
print(obj.string)
print(obj.get_text())

# 4.2 获取节点属性
obj = soup.select('#p1')[0]
# name 是标签的名字
print(obj.name)
# attrs 是标签的属性和属性值
print(obj.attrs) # {'id': 'p1', 'class': ['p1', 's1']}

# 获取节点属性值
attrs = obj.attrs
print(obj.get('class')) # 不推荐
print(obj.attrs.get('class')) # 推荐
print(attrs['class'])
print(attrs['class'][1])
```

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Title</title>
</head>
<body>
    <div>
    <ul>
        <li id="l1">张三</li>
        <li id="l2">李四</li>
        <li>王五</li>
        <a href="" id="123" class="a1">大茶馆</a>
        <span>哈哈哈哈</span>
    </ul>
        </div>
    <a href="" title="a2">网站</a>

    <div id="d1">
        <span>
            嘿嘿嘿嘿
        </span>
    </div>
    <p id="p1" class="p1 s1"></p>
</body>
</html>
```

### 爬取案例

```python
import urllib_base
import urllib.request
from bs4 import BeautifulSoup

url = 'https://www'

content = urllib_base.retrieve(url)
print(content)

soup = BeautifulSoup(content, 'lxml')

# //ul[@class="grid"]//strong/text()
raw_name_list = soup.select('ul[class="grid"] strong')
# name_list = [x.get_text() for x in raw_name_list]
name_list = list(map(lambda x: x.get_text(), raw_name_list))
print(name_list)
```

> 定位数据时，可先使用 xpath，然后将其语法转为 BeautifulSoup
{: .prompt-info }

