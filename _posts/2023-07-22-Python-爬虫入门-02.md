---
layout: post
title: Python-爬虫入门-02
date: 2023-07-22 16:45:30.000000000 +09:00
categories: [Python, 爬虫]
tags: [Python, 爬虫]
---

## 17 xpath

### 安装步骤

1. 安装 xpath
* chrome 安装 xpath helper 插件
* xpath 快捷键 `Ctrl + Shift + X`
2. 安装 lxml

```bash
$ pip3 install lxml
```

3. 导入 etree

```python
from lxml import etree
```

###  xpath 解析
1. 解析本地文件：使用 `etree.parse`
2. 解析服务器响应文件：`etree.HTML(response.read().decode('utf-8'))`   

### 解析本地文件

* html 

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <!-- 这里必须结束有个 /，否则 etree.parse 会失败-->
    <meta charset="UTF-8" />
    <title>Title</title>
</head>
<body>
    <ul>
        <li id="l1" class="c1">北京</li>
        <li id="l2">上海</li>
        <li id="c3" class="l3">深圳</li>
        <li id="c4">武汉</li>
    </ul>
</body>
</html>
```

* xpath 基本语法

```python
from lxml import etree

# xpath 解析本地文件
tree = etree.parse('17_解析_xpath_基本使用.html')
print(tree)

# tree.xpath('xpath 路径')

# xpath 基本语法
# 1. 路径查询
# // 查找所有子孙节点，不考虑层级关系
# / 查找直接子节点

# 查找 ul 下的 li
li_list = tree.xpath('//ul/li')
print(li_list)

# 2. 谓词查询
# //div[@id]
# //div[@id="maincontent"]

# 查找所有有 id 属性的 li 标签
#li_list = tree.xpath('//ul/li[@id]')

# 查找 id 为 l1 的 li 标签，要注意引号的问题
li_list = tree.xpath('//ul/li[@id="l1"]')


# 3. 属性查询
# //@class 获取 class 属性的值
# @value 获取 value 属性的值

# 查找 id=l1 的标签的 class 的属性值
cls = tree.xpath('//ul/li[@id="l1"]/@class')
print(cls)

# 4. 内容查询
# text() 获取标签中的内容
li_list = tree.xpath('//ul/li[@id]/text()')
print('contents = %s' % li_list)

# 5. 模糊查询（用的少）
# //div[contains(@id, "he")]
# //div[starts-with(@id, "he")]

# case1：包含用 contains，查询 id 包含 l 的标签
li_list = tree.xpath('//ul/li[contains(@id, "l")]/text()')
print('id.contains l list = %s' % li_list)

# case2：以什么开头用 starts-with
li_list = tree.xpath('//ul/li[starts-with(@id, "c")]/text()')
print('starts-with l list = %s' % li_list)


# 6. 逻辑运算（用的少）
# [@id="l1" and @class="c1"]，id 为 l1 并且 class 为 c1 的标签
# 查询 id=l1 或 class=l3 的 li 标签
li_list = tree.xpath('//ul/li[@id="l1" or @class="l3"]/text()')
print('logic l list = %s' % li_list)

# 或者这么写
li_list = tree.xpath('//ul/li[@id="l1"]/text() | //ul/li[@class="l3"]/text()')
print('logic l list 2 = %s' % li_list)
```


> 如果环境有问题可以点 Pycharm 右下角的 Python Interpreter 进行配置
{: .prompt-info }

### 利用 xpath 获取标签内 value 属性的内容

```python
from lxml import etree
import urllib.request
import ssl

# step 1 获取网页源码
# 解决 urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]，全局配置为不验证
ssl._create_default_https_context = ssl._create_unverified_context

url = 'https://'

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}

request = urllib.request.Request(url=url, headers=headers)

# 三步骤：handler、build_opener、open
# 使用 handler 后就可以使用代理了
# 1. 获取 handler 对象
handler = urllib.request.HTTPHandler()

# 2. 获取 opener 对象
opener = urllib.request.build_opener(handler)

# 3. 调用 open 方法
response = opener.open(request)
content = response.read().decode('utf-8')
print(content)

# step 2 解析响应信息 etree.HTML()
# 解析网页源码来获取想要的数据
tree = etree.HTML(content)
print(tree)


# <input type="submit" id="hh" value="哈哈" class="bg"></span>
# @value 获取 value 属性的值
# xpath 的返回值是一个列表类型的数据
results = tree.xpath('//input[@id="hh"]/@value')

# step 3 打印
print('value is %s' % results)
```

> Tips：可以使用 chrome 的 xpath 插件进行查询来验证 xpath 语法是否正确
{: .prompt-info }


### 利用 xpath 获取图片并下载到本地

```python
import urllib_base
import ssl
from lxml import etree
import urllib.request

ssl._create_default_https_context = ssl._create_unverified_context

# 下载 10 页图
# https://.html

def fetch_html(page):
    url = 'https://.html'
    if page > 1:
        # f-string 字符串格式化
        url = f'https://xxx_{page}.html'

    print(f'html url={url}')

    request = urllib_base.create_request(url)
    response = urllib_base.get_response(request)
    content = response.read().decode('UTF-8')
    # print(content)
    return content

def save(content, page):
    name = 'xxx_' + str(page) + '.html'
    with open(name, 'w', encoding='UTF-8') as fp:
        fp.write(content)

def parse(content):
    print('begin parse html content')
    tree = etree.HTML(content)

    # <img src="//www.xxx.com/a83_s.jpg" data-original="//xxx.jpg" class="lazy" alt="图片">
    
    img_urls = tree.xpath('//div[@class="item"]/img/@data-original')
    img_names = tree.xpath('//div[@class="item"]/img/@alt')
    print(f'img_names is {img_names}')
    print(f'img_urls is {img_urls}')

    for i in range(0, 3):
        print(f'idx is {i}')
        img_url = 'https:' + img_urls[i]
        img_name = './images/' + img_names[i] + '.jpg'
        urllib.request.urlretrieve(url=img_url, filename=img_name)


if __name__ == '__main__':
    start_page = int(input('请输入起始页'))
    end_page = int(input('请输入结束页'))

    for page in range(start_page, end_page + 1):
        content = fetch_html(page)
        save(content, page)
        parse(content)
```

> 获取的网页可能和浏览器里的属性不一样，最后获取到网页保存到本地查看需要的属性，这里 div 的 class 属性从很长变为了 item。另外，如果是图片的页面，通常有懒加载，url 可能会变，这里就从 src 变成了 data-original
{: .prompt-info }
