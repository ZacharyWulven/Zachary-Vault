---
layout: post
title: Python-爬虫入门-02
date: 2023-07-22 16:45:30.000000000 +09:00
categories: [Python, 爬虫]
tags: [Python, 爬虫]
---

## 17 xpath

### 安装步骤

1. 安装 xpath
* chrome 安装 xpath helper 插件
* xpath 快捷键 `Ctrl + Shift + X`
2. 安装 lxml

```bash
$ pip3 install lxml
```

3. 导入 etree

```python
from lxml import etree
```

###  xpath 解析
1. 解析本地文件：使用 `etree.parse`
2. 解析服务器响应文件：`etree.HTML(response.read().decode('utf-8'))`   

### 解析本地文件

* html 

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <!-- 这里必须结束有个 /，否则 etree.parse 会失败-->
    <meta charset="UTF-8" />
    <title>Title</title>
</head>
<body>
    <ul>
        <li id="l1" class="c1">北京</li>
        <li id="l2">上海</li>
        <li id="c3" class="l3">深圳</li>
        <li id="c4">武汉</li>
    </ul>
</body>
</html>
```

* xpath 基本语法

```python
from lxml import etree

# xpath 解析本地文件
tree = etree.parse('17_解析_xpath_基本使用.html')
print(tree)

# tree.xpath('xpath 路径')

# xpath 基本语法
# 1. 路径查询
# // 查找所有子孙节点，不考虑层级关系
# / 查找直接子节点

# 查找 ul 下的 li
li_list = tree.xpath('//ul/li')
print(li_list)

# 2. 谓词查询
# //div[@id]
# //div[@id="maincontent"]

# 查找所有有 id 属性的 li 标签
#li_list = tree.xpath('//ul/li[@id]')

# 查找 id 为 l1 的 li 标签，要注意引号的问题
li_list = tree.xpath('//ul/li[@id="l1"]')


# 3. 属性查询
# //@class 获取 class 属性的值
# @value 获取 value 属性的值

# 查找 id=l1 的标签的 class 的属性值
cls = tree.xpath('//ul/li[@id="l1"]/@class')
print(cls)

# 4. 内容查询
# text() 获取标签中的内容
li_list = tree.xpath('//ul/li[@id]/text()')
print('contents = %s' % li_list)

# 5. 模糊查询（用的少）
# //div[contains(@id, "he")]
# //div[starts-with(@id, "he")]

# case1：包含用 contains，查询 id 包含 l 的标签
li_list = tree.xpath('//ul/li[contains(@id, "l")]/text()')
print('id.contains l list = %s' % li_list)

# case2：以什么开头用 starts-with
li_list = tree.xpath('//ul/li[starts-with(@id, "c")]/text()')
print('starts-with l list = %s' % li_list)


# 6. 逻辑运算（用的少）
# [@id="l1" and @class="c1"]，id 为 l1 并且 class 为 c1 的标签
# 查询 id=l1 或 class=l3 的 li 标签
li_list = tree.xpath('//ul/li[@id="l1" or @class="l3"]/text()')
print('logic l list = %s' % li_list)

# 或者这么写
li_list = tree.xpath('//ul/li[@id="l1"]/text() | //ul/li[@class="l3"]/text()')
print('logic l list 2 = %s' % li_list)
```


> 如果环境有问题可以点 Pycharm 右下角的 Python Interpreter 进行配置
{: .prompt-info }

### 利用 xpath 获取标签内 value 属性的内容

```python
from lxml import etree
import urllib.request
import ssl

# step 1 获取网页源码
# 解决 urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]，全局配置为不验证
ssl._create_default_https_context = ssl._create_unverified_context

url = 'https://'

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}

request = urllib.request.Request(url=url, headers=headers)

# 三步骤：handler、build_opener、open
# 使用 handler 后就可以使用代理了
# 1. 获取 handler 对象
handler = urllib.request.HTTPHandler()

# 2. 获取 opener 对象
opener = urllib.request.build_opener(handler)

# 3. 调用 open 方法
response = opener.open(request)
content = response.read().decode('utf-8')
print(content)

# step 2 解析响应信息 etree.HTML()
# 解析网页源码来获取想要的数据
tree = etree.HTML(content)
print(tree)


# <input type="submit" id="hh" value="哈哈" class="bg"></span>
# @value 获取 value 属性的值
# xpath 的返回值是一个列表类型的数据
results = tree.xpath('//input[@id="hh"]/@value')

# step 3 打印
print('value is %s' % results)
```

> Tips：可以使用 chrome 的 xpath 插件进行查询来验证 xpath 语法是否正确
{: .prompt-info }


### 利用 xpath 获取图片并下载到本地

```python
import urllib_base
import ssl
from lxml import etree
import urllib.request

ssl._create_default_https_context = ssl._create_unverified_context

# 下载 10 页图
# https://.html

def fetch_html(page):
    url = 'https://.html'
    if page > 1:
        # f-string 字符串格式化
        url = f'https://xxx_{page}.html'

    print(f'html url={url}')

    request = urllib_base.create_request(url)
    response = urllib_base.get_response(request)
    content = response.read().decode('UTF-8')
    # print(content)
    return content

def save(content, page):
    name = 'xxx_' + str(page) + '.html'
    with open(name, 'w', encoding='UTF-8') as fp:
        fp.write(content)

def parse(content):
    print('begin parse html content')
    tree = etree.HTML(content)

    # <img src="//www.xxx.com/a83_s.jpg" data-original="//xxx.jpg" class="lazy" alt="图片">
    
    img_urls = tree.xpath('//div[@class="item"]/img/@data-original')
    img_names = tree.xpath('//div[@class="item"]/img/@alt')
    print(f'img_names is {img_names}')
    print(f'img_urls is {img_urls}')

    for i in range(0, 3):
        print(f'idx is {i}')
        img_url = 'https:' + img_urls[i]
        img_name = './images/' + img_names[i] + '.jpg'
        urllib.request.urlretrieve(url=img_url, filename=img_name)


if __name__ == '__main__':
    start_page = int(input('请输入起始页'))
    end_page = int(input('请输入结束页'))

    for page in range(start_page, end_page + 1):
        content = fetch_html(page)
        save(content, page)
        parse(content)
```

> 获取的网页可能和浏览器里的属性不一样，最后获取到网页保存到本地查看需要的属性，这里 div 的 class 属性从很长变为了 item。另外，如果是图片的页面，通常有懒加载，url 可能会变，这里就从 src 变成了 data-original
{: .prompt-info }


## 18 jsonpath
* jsonpath 只能解析本地文件

### 安装 jsonpath

```bash
$ pip3 install jsonpath
```

### 基本语法 JsonPath VS XPath

```
XPath  JsonPath          说明
/      $                 文档根元素
.      @                 当前元素
/      . 或 []           匹配下级元素
..     N/A               匹配上级元素，JsonPath不支持此操作符
//     ..                递归匹配所有子元素
*      *                 通配符，匹配下级元素
@      N/A               匹配属性，JsonPath不支持此操作符
[]     []                下标运算符，根据索引获取元素，XPath索引从1开始，JsonPath索引从0开始
|      [,]               连接操作符，将多个结果拼接成数组返回，可以使用索引或别名
N/A    [start:end:step]  数据切片操作，XPath不支持
[]     ?()               过滤表达式
N/A    ()                脚本表达式，使用底层脚本引擎，XPath不支持
()     N/A               分组，JsonPath不支持

```

### 例子

```json
{
    "store": {
        "book": [{
                "category": "reference",
                "author": "Nigel Rees",
                "title": "Sayings of the Century",
                "price": 8.95
            }, {
                "category": "fiction",
                "author": "Evelyn Waugh",
                "title": "Sword of Honour",
                "price": 12.99
            }, {
                "category": "fiction",
                "author": "Herman Melville",
                "title": "Moby Dick",
                "isbn": "0-553-21311-3",
                "price": 8.99
            }, {
                "category": "fiction",
                "author": "J. R. R. Tolkien",
                "title": "The Lord of the Rings",
                "isbn": "0-395-19395-8",
                "price": 22.99
            }
        ],
        "bicycle": {
            "color": "red",
            "price": 19.95
        }
    }
}
```


* 例子代码

```
XPath                  JsonPath                               Result
/store/book/author     $.store.book[*].author                 所有 book 的 author 节点
//author               $..author                              所有 author 节点
/store/*               $.store.*                              store 下的所有节点，book 数组和 bicycle 节点
/store//price          $.store..price                         store 下的所有 price 节点
//book[3]              $..book[2]                             匹配第 3 个 book 节点
//book[last()]         $..book[(@.length-1)]，或 $..book[-1:]  匹配倒数第 1 个 book 节点
//book[position()<3]   $..book[0,1]，或 $..book[:2]            匹配前两个 book 节点
//book[isbn]           $..book[?(@.isbn)]                     过滤含 isbn 字段的节点
//book[price<10]       $..book[?(@.price<10)]                 过滤 price<10 的节点
//*                    $..*                                   递归匹配所有子节点
```


```python
# 导入 jsonpath
import json

import jsonpath

obj = json.load(open('jsonpath_store.json', 'r', encoding='utf-8'))
print(obj)

# 1 书店所有书的作者
# book[*] 表示所有的书
# book[0] 表示第一本书
authors = jsonpath.jsonpath(obj, '$.store.book[*].author')
print(authors)

first_author = jsonpath.jsonpath(obj, '$.store.book[0].author')
print(first_author)

# 2 所有的作者
all_authors = jsonpath.jsonpath(obj, '$..author')
print(all_authors)

# 3 store 下所有元素
tag_list = jsonpath.jsonpath(obj, '$.store.*')
print(tag_list)

# 4 store 下所有的 price
price_list = jsonpath.jsonpath(obj, '$.store..price')
print(price_list)

# 5 第三本书
third_book = jsonpath.jsonpath(obj, '$..book[2]')
print(third_book)


# 6 最后一本书
# last_book = jsonpath.jsonpath(obj, '$..book[(@.length-1)]')
last_book = jsonpath.jsonpath(obj, '$..book[-1:]')
print(last_book)

# 7 前两本书
# books = jsonpath.jsonpath(obj, '$..book[0,1]')
books = jsonpath.jsonpath(obj, '$..book[:2]')
print(books)

# 8 所有包含 isbn 的书
# Tips: 条件过滤，需要在 () 前加一个 ?
books = jsonpath.jsonpath(obj, '$..book[?(@.isbn)]')
print(books)

# 9 哪些书超过了 10 块钱
books = jsonpath.jsonpath(obj, '$..book[?(@.price > 10)]')
print(books)
```

[参考链接](https://www.jianshu.com/p/9808ab64fc0c)
