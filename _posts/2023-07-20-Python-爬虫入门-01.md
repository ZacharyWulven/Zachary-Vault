---
layout: post
title: Python-爬虫入门-01
date: 2023-07-20 16:45:30.000000000 +09:00
categories: [Python, 爬虫]
tags: [Python, 爬虫]
---

# urllib
## 1 urllib 基本使用 

```python
# 使用 urllib 获取指定 url 源码

import urllib.request

# 定义 url
url = ''
print(url)

# 模拟浏览器向服务器发送请求
response = urllib.request.urlopen(url)

# 获取响应中的页面源码
# read 方法返回的是字节形式的二进制数据
# 二进制 --> 字符串，称为解码 decode('编码格式')
content = response.read().decode('utf-8')
print(content)
```

## 2 urllib 一个类型、六个方法
* 类型：response 是 HTTPResponse 类型
* 方法
1. read
2. readline
3. readlines
4. getcode
5. geturl
6. getheaders


```python

import urllib.request

# 定义 url
url = ''

# 模拟浏览器向服务器发送请求
response = urllib.request.urlopen(url)

# urllib 一个类型、六个方法
# response 是 HTTPResponse 类型
print(type(response))

# 方法 1：read，按照一个字节一个字节去读
#content = response.read()

# 读 5 个字节
# content = response.read(5)

# 方法 2：读取一行
# content = response.readline()

# 方法 3：按行读，直到没有数据
# content = response.readlines()
# print(content)

# 方法 4：返回状态码，如果是 200 就证明代码没有问题
print(response.getcode())

# 方法 5：返回 url 地址
print(response.geturl())

# 方法 6：获取状态信息
print(response.getheaders())
```


## 3 使用 urllib 下载

```python
import urllib.request

# 下载网页
url_page = ''

# urlretrieve 参数
# 1 url 是下载的路径
# 2 filename 是下载的文件名字
urllib.request.urlretrieve(url_page, 'page.html')

# 下载图片
jiexika = 'https://gimg2.xxx.com/image_search/src=http%3A%2F%2Fsafe-img.xhscdn.com%2Fbw1%2F8facfcf5-2108-46e1-8fb1-0773f401ab44%3FimageView2%2F2%2Fw%2F1080%2Fformat%2Fjpg&refer=http%3A%2F%2Fsafe-img.xhscdn.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1693067282&t=a45fb05d629f0faef9b486886e021f8c'
urllib.request.urlretrieve(url=jiexika, filename='jiexika.jpg')
```

## 4 请求对象定制
* 解决 UA 反爬
* 反爬点 1：header 里的 UA


```python
import urllib.request

url = ''

# url 组成
# http/https   www.hogetsu.com   80/443                 #
# 协议          主机               端口号    路径   参数    锚点
# http 80
# https 443
# mysql 3306
# oracle 1521
# redis 6379
# mongodb 27017


# UA 即 UserAgent，它是一种特殊的字符串头，使得服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本、浏览器内核等
# 解决 UA 反爬
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}

# 请求对象的定制
# 由于 urlopen 的参数没有字典类型，所以需要构建一个 request 对象
# Note 因为参数顺序问题，不能直接写 url 和 header，要使用参数关键字
request = urllib.request.Request(url=url, headers=headers)
response = urllib.request.urlopen(request)
content = response.read().decode('utf-8')
print(content)
```

## 5 urllib get 请求的 quote 方法
* 编解码 urllib.parse.quote 方法：将内容转为 Unicode 码

```python
name = urllib.parse.quote('')
print('name is %s', name)
```

## 6 urllib get 请求的 urlencode 方法 
* quote 方法只能将一个参数转为 Unicode 码，可以改为使用 urlencode
* 使用方式：需要将参数设置为一个字典

```python
data = {
    'wd': '张三',
    'sex': '男',
}

query = urllib.parse.urlencode(data)

print('query is %s' % query)
```


## 7 Issue: 解决 SSL 证书问题
* 问题描述：`urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)>`
* 传入 context 解决

```python
import ssl
context = ssl._create_unverified_context()
request = urllib.request.Request(url=url, data=data, headers=headers)
response = urllib.request.urlopen(request, context=context)
```

## 8 urllib post 请求的
* post 请求参数需要最后 encode 为 byte 类型

```python
import urllib.request
import urllib.parse
import ssl

url = ''

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}

# post 请求的参数，必须进行编码，即必须使用 urlencode
data = {
    'keyword': 'spider'
}

# encode('utf-8') 变为 byte 类型, 因为 post 参数需要时 byte 类型
data = urllib.parse.urlencode(data).encode('utf-8')
print(data)

request = urllib.request.Request(url=url, data=data, headers=headers)

context = ssl._create_unverified_context()
response = urllib.request.urlopen(request, context=context)
content = response.read().decode('utf-8')
print(content)
print(type(content))

# 字符串 -> json 对象
import json
obj = json.loads(content)
# <class 'dict'>
print(type(obj))
```

## 9 request header 里的 Cookie 反爬
* 有些接口设置了反爬，所以可能 header 需要更多字段
* 这里的反爬是 Cookie 字段，只要设置了 Cookie 字段就能成功
* 反爬点 2：header 里的 Cookie

```python
import urllib.request
import urllib.parse
import ssl

url = ''

# 有些接口设置了反爬 所以可能 header 需要更多字段
# 这里的反爬是 Cookie 字段，只要设置了 Cookie 字段就能成功

headers = {
# 'Accept': '*/*',

# 写爬虫时候这个参数一定要注释掉
# 'Accept-Encoding': 'gzip, deflate, br',

# 'Accept-Language': 'zh-CN,zh;q=0.9',
# 'Acs-Token': '1690542055327_169054229232',
# 'Authorization': '400b032e-9f52-4cb9-8de2-18818e3ff3ac',
# 'Connection': 'keep-alive',
# 'Content-Length': '152',
# 'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
'Cookie': 'SID=713FD8222BC9D82CBED063C645D5908222BC9D82CEBD9D6D7041B12A7',
# 'Host': '',
# 'M-Language': 'zh-cn',
# 'Origin': 'https://',
# 'Referer': 'https:///?aldtype=16047',
# 'Sec-Ch-Ua': "Not.A/Brand';v='8', 'Chromium';v='114', 'Google Chrome';v='114",
# 'Sec-Ch-Ua-Mobile': '?0',
# 'Sec-Ch-Ua-Platform': "macOS",
# 'Sec-Fetch-Dest': 'empty',
# 'Sec-Fetch-Mode': 'cors',
# 'Sec-Fetch-Site': 'same-origin',
# 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
# 'X-Requested-With': 'XMLHttpRequest'
}

# post 接口参数
data = {
'from': '',
'to': 'zh',
'query': '',
}

# encode 参数
data = urllib.parse.urlencode(data).encode('utf-8')

request = urllib.request.Request(url=url, data=data, headers=headers)

# 处理 ssl 证书校验
context = ssl._create_unverified_context()
response = urllib.request.urlopen(request, context=context)
content = response.read().decode('utf-8')

import json
obj = json.loads(content)
print(obj)
```

> 写爬虫时候 Accept-Encoding 这个参数一定要注释掉
{: .prompt-info }


## 10 ajax get 请求

### urllib_base 封装
 
```python
import urllib.request
import urllib.parse
import ssl
import json


def retrieve(base_url, method='GET', data=None, headers=None, verify_ssl=False):
    url = base_url

    # 处理 UA
    if headers is None:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) '
                          'Chrome/114.0.0.0 Safari/537.36'
        }

    # 处理参数
    if method == 'POST':
        if data is not None:
            data = urllib.parse.urlencode(data).encode('utf-8')
    elif method == 'GET':
        if data is None:
            url = base_url
        else:
            data = urllib.parse.urlencode(data)
            url = base_url + data
            data = None

    request = urllib.request.Request(url=url, data=data, headers=headers)

    # M2 mac 不需要 context
    context = None
    if verify_ssl is False:
        context = ssl._create_unverified_context()

    response = urllib.request.urlopen(request, context=context)
    content = response.read().decode('utf-8')
    return content


# 字符串 -> json 对象
def to_json(content=''):
    obj = json.loads(content)
    return obj


if __name__ == '__main__':
    print('call urllib base')
```

### 获取一页的数据

```python
import urllib_base

# 1 get 请求
url = ''

# 2 获取响应数据
content = urllib_base.retrieve(url)
json = urllib_base.to_json(content)
print(json)

# 3 下载到本地
# 写法一
# fp = open('xxx.json', 'w', encoding='utf-8')
# fp.write(content)

# 写法二
with open('xxx.json', 'w', encoding='utf-8') as fp:
    fp.write(content)
```

> Command + Option + L，可以将 .json 文件格式化，看起更直观
{: .prompt-info }

### 获取多页的数据

```python
import urllib_base

def download_movie(page, limit=20):
    url = ''
    data = {
        'start': (page - 1) * limit,
        'limit': limit,
    }
    content = urllib_base.retrieve(url, data=data)
    json = urllib_base.to_json(content)
    print(json)
    file_name = 'movie_' + str(page) + '.json'
    with open(file_name, 'w', encoding='utf-8') as fp:
        fp.write(content)


def download():
    start_page = int(input('请输入起始页'))
    end_page = int(input('请输入结束页'))

    for page in range(start_page, end_page + 1):
        download_movie(page)


if __name__ == '__main__':
    download()
```

> 当 Request Header 里有 `'X-Requested-With': 'XMLHttpRequest'` 说明它是要给 ajax 请求
{: .prompt-info }


## 11 ajax post 请求

```python
import urllib_base

def download(page=1, limit=10):
    url = ''

    data = {
        'cname': '北京',
        'pid': '',
        'pageIndex': page,
        'pageSize': 10,
    }

    content = urllib_base.retrieve(url, method='POST', data=data)
    json = urllib_base.to_json(content)
    file_name = 'xxx_' + str(page) + '.json'
    urllib_base.save_to_file(file_name, content)


if __name__ == '__main__':
    start_page = int(input('请输入起始页'))
    end_page = int(input('请输入结束页'))

    for page in range(start_page, end_page + 1):
        download(page)
```

## 12 urllib 异常

```python
import urllib.request
import urllib.parse
import urllib.error

url = 'http://www.dou111.com'

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/114.0.0.0 Safari/537.36'
}

# 可以使用 try 对异常进行捕获
try:
    request = urllib.request.Request(url=url, headers=headers)
    response = urllib.request.urlopen(request)
    content = response.read().decode('utf-8')
    print(content)
# 捕获的是 HTTPError
except urllib.error.HTTPError:
    print('HTTPError')
# 捕获的是 URLError, 这个异常一般是域名或参数有问题
except urllib.error.URLError:
    print('URLError')
```

## 13 Request Header 中的 Cookie 登录
* 使用场景：在数据采集时，需要绕过登录，然后进入到某个页面
* Header 中参数
1. cookie 中携带了你的登录信息，有了登录后的 Cookie 就可以进入任何页面
2. referer 也可以对爬虫进行限制，是用于防盗链，它判定是不是由上一个路径进来的，一般是做图片的防盗链


```python
import urllib_base
import urllib

url = 'https://xxx.com/profile/info?uid=123456'

# 未登录
# 个人信息页是 utf-8 编码，但没有登录跳转到了登录页，而登录页不是 utf-8
# request = urllib_base.create_request(url)
# response = urllib.request.urlopen(request)
# content = response.read()
# print(content)

# 登录
# 设置 cookie 设置登录
headers = {
    'Cookie': '28f0be5504;',
    'referer': 'https://xxxxx/u/123456',
}

content = urllib_base.retrieve(url, headers=headers)
print(content)
```


> Tips: Request Header 中的冒号开头的都不是有效信息，例如 `:Authority`；有些网站时而验证 referer，时而不验证
{: .prompt-info }

## 14 urllib handler 使用
1. 创建 handler 对象
2. 获取 opener 对象
3. 调用 open 方法


```python
# 使用 handler 访问页面源码
import urllib.request
import ssl

# 解决 urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]，全局配置为不验证
ssl._create_default_https_context = ssl._create_unverified_context

url = 'http://'

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}

request = urllib.request.Request(url=url, headers=headers)

# 三步骤：handler、build_opener、open
# 使用 handler 后就可以使用代理了
# 1. 获取 handler 对象
handler = urllib.request.HTTPHandler()

# 2. 获取 opener 对象
opener = urllib.request.build_opener(handler)

# 3. 调用 open 方法
response = opener.open(request)
content = response.read().decode('utf-8')
print(content)
```

## 15 handler 代理
* 可在一些代理网站找免费的 ip 使用或购买，比如 `https://www.k #u *a &i $d {a }i {l @i .com/free/`

```python
import urllib.request
import ssl

# 解决 urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]，全局配置为不验证
ssl._create_default_https_context = ssl._create_unverified_context

url = 'http:'

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}

request = urllib.request.Request(url=url, headers=headers)
# context = ssl._create_unverified_context()
# response = urllib.request.urlopen(request, context=context)


# 代理参数是一个字典
proxies = {
    # 'http': '183.236.232.160:8080'
    'http': '144.255.48.238:9999'
    # 'http': '118.24.219.151:16817'
}

handler = urllib.request.ProxyHandler(proxies=proxies)

# 2. 获取 opener 对象
opener = urllib.request.build_opener(handler)

# 3. 调用 open 方法
response = opener.open(request)
content = response.read().decode('utf-8')
print(content)


with open('daili.html', 'w', encoding='utf-8') as fp:
    fp.write(content)
```

## 16 代理池
* 使用 random 的 choice 方法随机选出一个代理


```python
import random
import urllib.request
import ssl

ssl._create_default_https_context = ssl._create_unverified_context

proxy_pool = [
    {'http': '14:9999'},
    {'http': '14.2:8888'},
]

# 在代理池中随机选一个代理
proxies = random.choice(proxy_pool)
print(proxies)

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}

url = 'http:'

request = urllib.request.Request(url=url, headers=headers)
handler = urllib.request.ProxyHandler(proxies=proxies)
opener = urllib.request.build_opener(handler)
response = opener.open(request)
print(response)
content = response.read().decode('utf-8')
print(content)

with open('2.html', 'w', encoding='utf-8') as fp:
    fp.write(content)
```
