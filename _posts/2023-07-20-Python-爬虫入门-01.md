---
layout: post
title: Python-爬虫入门-01
date: 2023-07-20 16:45:30.000000000 +09:00
categories: [Python, 爬虫]
tags: [Python, 爬虫]
---

# urllib
## 1 urllib 基本使用 

```python
# 使用 urllib 获取指定 url 源码

import urllib.request

# 定义 url
url = ''
print(url)

# 模拟浏览器向服务器发送请求
response = urllib.request.urlopen(url)

# 获取响应中的页面源码
# read 方法返回的是字节形式的二进制数据
# 二进制 --> 字符串，称为解码 decode('编码格式')
content = response.read().decode('utf-8')
print(content)
```

## 2 urllib 一个类型、六个方法
* 类型：response 是 HTTPResponse 类型
* 方法
1. read
2. readline
3. readlines
4. getcode
5. geturl
6. getheaders


```python

import urllib.request

# 定义 url
url = ''

# 模拟浏览器向服务器发送请求
response = urllib.request.urlopen(url)

# urllib 一个类型、六个方法
# response 是 HTTPResponse 类型
print(type(response))

# 方法 1：read，按照一个字节一个字节去读
#content = response.read()

# 读 5 个字节
# content = response.read(5)

# 方法 2：读取一行
# content = response.readline()

# 方法 3：按行读，直到没有数据
# content = response.readlines()
# print(content)

# 方法 4：返回状态码，如果是 200 就证明代码没有问题
print(response.getcode())

# 方法 5：返回 url 地址
print(response.geturl())

# 方法 6：获取状态信息
print(response.getheaders())
```


## 3 使用 urllib 下载

```python
import urllib.request

# 下载网页
url_page = ''

# urlretrieve 参数
# 1 url 是下载的路径
# 2 filename 是下载的文件名字
urllib.request.urlretrieve(url_page, 'page.html')

# 下载图片
jiexika = 'https://gimg2.xxx.com/image_search/src=http%3A%2F%2Fsafe-img.xhscdn.com%2Fbw1%2F8facfcf5-2108-46e1-8fb1-0773f401ab44%3FimageView2%2F2%2Fw%2F1080%2Fformat%2Fjpg&refer=http%3A%2F%2Fsafe-img.xhscdn.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1693067282&t=a45fb05d629f0faef9b486886e021f8c'
urllib.request.urlretrieve(url=jiexika, filename='jiexika.jpg')
```

## 4 请求对象定制
* 解决 UA 反爬
* 反爬点 1：header 里的 UA


```python
import urllib.request

url = ''

# url 组成
# http/https   www.hogetsu.com   80/443                 #
# 协议          主机               端口号    路径   参数    锚点
# http 80
# https 443
# mysql 3306
# oracle 1521
# redis 6379
# mongodb 27017


# UA 即 UserAgent，它是一种特殊的字符串头，使得服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本、浏览器内核等
# 解决 UA 反爬
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}

# 请求对象的定制
# 由于 urlopen 的参数没有字典类型，所以需要构建一个 request 对象
# Note 因为参数顺序问题，不能直接写 url 和 header，要使用参数关键字
request = urllib.request.Request(url=url, headers=headers)
response = urllib.request.urlopen(request)
content = response.read().decode('utf-8')
print(content)
```

## 5 urllib get 请求的 quote 方法
* 编解码 urllib.parse.quote 方法：将内容转为 Unicode 码

```python
name = urllib.parse.quote('')
print('name is %s', name)
```

## 6 urllib get 请求的 urlencode 方法 
* quote 方法只能将一个参数转为 Unicode 码，可以改为使用 urlencode
* 使用方式：需要将参数设置为一个字典

```python
data = {
    'wd': '张三',
    'sex': '男',
}

query = urllib.parse.urlencode(data)

print('query is %s' % query)
```


## 7 Issue: 解决 SSL 证书问题
* 问题描述：`urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)>`
* 传入 context 解决

```python
import ssl
context = ssl._create_unverified_context()
request = urllib.request.Request(url=url, data=data, headers=headers)
response = urllib.request.urlopen(request, context=context)
```

## 8 urllib post 请求的
* post 请求参数需要最后 encode 为 byte 类型

```python
import urllib.request
import urllib.parse
import ssl

url = ''

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
}

# post 请求的参数，必须进行编码，即必须使用 urlencode
data = {
    'keyword': 'spider'
}

# encode('utf-8') 变为 byte 类型, 因为 post 参数需要时 byte 类型
data = urllib.parse.urlencode(data).encode('utf-8')
print(data)

request = urllib.request.Request(url=url, data=data, headers=headers)

context = ssl._create_unverified_context()
response = urllib.request.urlopen(request, context=context)
content = response.read().decode('utf-8')
print(content)
print(type(content))

# 字符串 -> json 对象
import json
obj = json.loads(content)
# <class 'dict'>
print(type(obj))
```

## 9 request header 里的 Cookie 反爬
* 有些接口设置了反爬，所以可能 header 需要更多字段
* 这里的反爬是 Cookie 字段，只要设置了 Cookie 字段就能成功
* 反爬点 2：header 里的 Cookie

```python
import urllib.request
import urllib.parse
import ssl

url = ''

# 有些接口设置了反爬 所以可能 header 需要更多字段
# 这里的反爬是 Cookie 字段，只要设置了 Cookie 字段就能成功

headers = {
# 'Accept': '*/*',

# 写爬虫时候这个参数一定要注释掉
# 'Accept-Encoding': 'gzip, deflate, br',

# 'Accept-Language': 'zh-CN,zh;q=0.9',
# 'Acs-Token': '1690542055327_169054229232',
# 'Authorization': '400b032e-9f52-4cb9-8de2-18818e3ff3ac',
# 'Connection': 'keep-alive',
# 'Content-Length': '152',
# 'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
'Cookie': 'SID=713FD8222BC9D82CBED063C645D5908222BC9D82CEBD9D6D7041B12A7',
# 'Host': '',
# 'M-Language': 'zh-cn',
# 'Origin': 'https://',
# 'Referer': 'https:///?aldtype=16047',
# 'Sec-Ch-Ua': "Not.A/Brand';v='8', 'Chromium';v='114', 'Google Chrome';v='114",
# 'Sec-Ch-Ua-Mobile': '?0',
# 'Sec-Ch-Ua-Platform': "macOS",
# 'Sec-Fetch-Dest': 'empty',
# 'Sec-Fetch-Mode': 'cors',
# 'Sec-Fetch-Site': 'same-origin',
# 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
# 'X-Requested-With': 'XMLHttpRequest'
}

# post 接口参数
data = {
'from': '',
'to': 'zh',
'query': '',
}

# encode 参数
data = urllib.parse.urlencode(data).encode('utf-8')

request = urllib.request.Request(url=url, data=data, headers=headers)

# 处理 ssl 证书校验
context = ssl._create_unverified_context()
response = urllib.request.urlopen(request, context=context)
content = response.read().decode('utf-8')

import json
obj = json.loads(content)
print(obj)
```

> 写爬虫时候 Accept-Encoding 这个参数一定要注释掉
{: .prompt-info }


