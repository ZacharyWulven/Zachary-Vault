---
layout: post
title: Python-爬虫入门-04
date: 2023-07-26 16:45:30.000000000 +09:00
categories: [Python, 爬虫]
tags: [Python, 爬虫]
---

# 22 Scrapy

## 简介
* Scrapy 是一个为了爬取网站数据，提取结构性数据而编写的框架。可以应用在数据挖掘、信息处理或存储历史数据等程序中

## 安装

```bash
pip3 install scrapy
```

## 基本使用

### 1 创建项目

```bash
scrapy startproject 项目名称
```

### 2 创建爬虫文件
* 在项目名称/项目名称/spiders 文件夹下创建爬虫文件
* 创建爬虫文件命令

```bash
scrapy genspider 爬虫文件名称 要爬取的网页

#例
scrapy genspider mouzhan www.moumouzhan.com
```

> 一般情况下，不需要加 http 协议前缀，因为 start_urls 是根据 allowed_domains 改变的，start_urls 为 allowed_domains 前边拼上 https://，后边拼上 /
{: .prompt-info }

### 3 运行爬虫代码

```bash
scrapy crawl 爬虫名称

#例
scrapy crawl moumouzhan
```

> 运行后可能有反爬，可访问 `https://www.xxxx.com/robots.txt`，这叫君子协定，不让爬。可修改 `settings.py` 里边有 `ROBOTSTXT_OBEY = True`，将其注释即可。也就不遵守 `robots` 协议了。
{: .prompt-info }


## Scrapy 项目结构

```
项目名称
  |--项目名称
    |--spiders       文件夹，存储的是爬虫文件
    |  |--__init__.py
    |  |--自定义的爬虫文件，即核心功能文件，是最重要的
    |
    |--__init__.py
    |--items.py       定义数据结构的地方，爬取的数据包含哪些
    |--middlewares.py 中间件，未来要实现的代理机制
    |--pipelines.py   管道，用于处理下载的数据
    |--settings,py    配置文件，例如 robos 协议、UA 等在这里定义
```

## Scrapy 自定义脚本的 parse 方法的 response 参数的属性和方法

* 获取的响应的字符串

```python
response.text
```

* 获取的是二进制数据

```python
response.body
```

* 使用 xpath 语法解析 response 内容（常用）
* extract 提取 selector 对象的 data 属性值（常用）

```python
# span 返回的是 Selector 对象
span = response.xpath('//div[@id="filter"]//span')[0]

# extract 获取 Selector 对象的 data 属性的值
span_text = span.extract()
```

* 提取 selector 列表里的第一个数据（常用）

```python
response.extract_first()
```

## scrapy 架构

### 例子

```python
import scrapy

class XXXSpider(scrapy.Spider):
    name = "xxx"
    allowed_domains = ["www.xxx.com"]
    # 如果请求的 url 是 html 为结尾的，则最后不能加 /
    start_urls = ["https://www.xxx/"]

    def parse(self, response):
        # pass

        # name_list 里是 Selector 对象
        name_list = response.xpath('//a[@class="xxx-info"]//h4[@class="xxx-name"]/text()')
        # print(name_list)
        print('name -----------------------------------------')
        # for name in name_list:
        #     print(name.extract())

        print('price -----------------------------------------')
        price_list = response.xpath('//a[@class="info"]//span//em/text()')
        # for price in price_list:
        #     print(price.extract())

        for i in range(len(name_list)):
            name = name_list[i].extract()
            price = price_list[i].extract()
            print(f'name={name}, price={price}')
```

> 如果请求的 url 是 html 为结尾的，则最后不能加 `/`
{: .prompt-info }

