---
layout: post
title: Python-爬虫入门-04
date: 2023-07-26 16:45:30.000000000 +09:00
categories: [Python, 爬虫]
tags: [Python, 爬虫]
---

# 22 Scrapy

## 简介
* Scrapy 是一个为了爬取网站数据，提取结构性数据而编写的框架。可以应用在数据挖掘、信息处理或存储历史数据等程序中

## 安装

```bash
pip3 install scrapy
```

## 基本使用

### 1 创建项目

```bash
scrapy startproject 项目名称
```

### 2 创建爬虫文件
* 在项目名称/项目名称/spiders 文件夹下创建爬虫文件
* 创建爬虫文件命令

```bash
scrapy genspider 爬虫文件名称 要爬取的网页

#例
scrapy genspider mouzhan www.moumouzhan.com
```

> 一般情况下，不需要加 http 协议前缀，因为 start_urls 是根据 allowed_domains 改变的，start_urls 为 allowed_domains 前边拼上 https://，后边拼上 /
{: .prompt-info }

### 3 运行爬虫代码

```bash
scrapy crawl 爬虫名称

#例
scrapy crawl moumouzhan
```

> 运行后可能有反爬，可访问 `https://www.xxxx.com/robots.txt`，这叫君子协定，不让爬。可修改 `settings.py` 里边有 `ROBOTSTXT_OBEY = True`，将其注释即可。也就不遵守 `robots` 协议了。
{: .prompt-info }


## Scrapy 项目结构

```
项目名称
  |--项目名称
    |--spiders       文件夹，存储的是爬虫文件
    |  |--__init__.py
    |  |--自定义的爬虫文件，即核心功能文件，是最重要的
    |
    |--__init__.py
    |--items.py       定义数据结构的地方，爬取的数据包含哪些
    |--middlewares.py 中间件，未来要实现的代理机制
    |--pipelines.py   管道，用于处理下载的数据
    |--settings,py    配置文件，例如 robos 协议、UA 等在这里定义
```

## Scrapy 自定义脚本的 parse 方法的 response 参数的属性和方法

* 获取的响应的字符串

```python
response.text
```

* 获取的是二进制数据

```python
response.body
```

* 使用 xpath 语法解析 response 内容（常用）

```python
span = response.xpath('//div[@id="filter"]//span')[0]
span_text = span.extract()
```

* 提取 selector 对象的 data 属性值（常用）

```python
response.extract()
```

* 提取 selector 列表里的第一个数据（常用）

```python
response.extract_first()
```
