---
layout: post
title: Python-爬虫入门-04
date: 2023-07-26 16:45:30.000000000 +09:00
categories: [Python, 爬虫]
tags: [Python, 爬虫]
---

# 22 Scrapy

## 简介
* Scrapy 是一个为了爬取网站数据，提取结构性数据而编写的框架。可以应用在数据挖掘、信息处理或存储历史数据等程序中

## 安装

```bash
pip3 install scrapy
```

## 基本使用

### 1 创建项目

```bash
scrapy startproject 项目名称
```

### 2 创建爬虫文件
* 在项目名称/项目名称/spiders 文件夹下创建爬虫文件
* 创建爬虫文件命令

```bash
scrapy genspider 爬虫文件名称 要爬取的网页

#例
scrapy genspider mouzhan www.moumouzhan.com
```

> 一般情况下，不需要加 `http` 协议前缀，因为 `start_urls` 是根据 `allowed_domains` 改变的，`start_urls` 为 `allowed_domains` 前边拼上 `https://`，后边拼上 `/`
{: .prompt-info }

### 3 运行爬虫代码

```bash
scrapy crawl 爬虫名称

#例
scrapy crawl moumouzhan
```

> 运行后可能有反爬，可访问 `https://www.xxxx.com/robots.txt`，这叫君子协定，不让爬。可修改 `settings.py` 里边有 `ROBOTSTXT_OBEY = True`，将其注释即可。也就不遵守 `robots` 协议了。
{: .prompt-info }


## Scrapy 项目结构

```
项目名称
  |--项目名称
    |--spiders       文件夹，存储的是爬虫文件
    |  |--__init__.py
    |  |--自定义的爬虫文件，即核心功能文件，是最重要的
    |
    |--__init__.py
    |--items.py       定义数据结构的地方，爬取的数据包含哪些
    |--middlewares.py 中间件，未来要实现的代理机制
    |--pipelines.py   管道，用于处理下载的数据
    |--settings,py    配置文件，例如 robos 协议、UA 等在这里定义
```

## Scrapy 自定义脚本的 parse 方法的 response 参数的属性和方法

* 获取的响应的字符串

```python
response.text
```

* 获取的是二进制数据

```python
response.body
```

* 使用 xpath 语法解析 response 内容（常用）
* extract 提取 selector 对象的 data 属性值（常用）

```python
# span 返回的是 Selector 对象
span = response.xpath('//div[@id="filter"]//span')[0]

# extract 获取 Selector 对象的 data 属性的值
span_text = span.extract()
```

* 提取 selector 列表里的第一个数据（常用）

```python
response.extract_first()
```

### 例子

```python
import scrapy

class XXXSpider(scrapy.Spider):
    name = "xxx"
    allowed_domains = ["www.xxx.com"]
    # 如果请求的 url 是 html 为结尾的，则最后不能加 /
    start_urls = ["https://www.xxx/"]

    def parse(self, response):
        # pass

        # name_list 里是 Selector 对象
        name_list = response.xpath('//a[@class="xxx-info"]//h4[@class="xxx-name"]/text()')
        # print(name_list)
        print('name -----------------------------------------')
        # for name in name_list:
        #     print(name.extract())

        print('price -----------------------------------------')
        price_list = response.xpath('//a[@class="info"]//span//em/text()')
        # for price in price_list:
        #     print(price.extract())

        for i in range(len(name_list)):
            name = name_list[i].extract()
            price = price_list[i].extract()
            print(f'name={name}, price={price}')
```

> 如果请求的 url 是 html 为结尾的，则最后不能加 `/`
{: .prompt-info }


## Scrapy 工作原理
1. 引擎向 spiders（爬虫文件）要 url，引擎将要爬取的 url 给调度器
2. 调度器获取 url 生成请求对象放到队列里
3. 从队列里出列一个请求给引擎，引擎将这个请求给到下载器
4. 下载器发送请求去互联网获取数据
3. 下载器将请求的数据返回给引擎，引擎把数据再给到 spiders 进行解析，解析数据分两种情况
* 如果是 url 会在回到第一步
* 如果是数据，就交给管道进行下载了


![image](/assets/images/creep/scrapy.png)


## Scrapy Shell
* Scrapy终端是一个交互终端，供您在未启动spider的情况下尝试及调试您的爬取代码。 其本意是用来测试提取数据的代码，不过您可以将其作为正常的Python终端，在上面测试任何的Python代码。
该终端是用来测试XPath或CSS表达式，查看他们的工作方式及从爬取的网页中提取的数据。 在编写您的spider时，该终端提供了交互性测试您的表达式代码的功能，免去了每次修改后运行spider的麻烦。
一旦熟悉了Scrapy终端后，您会发现其在开发和调试spider时发挥的巨大作用。
如果您安装了 IPython ，Scrapy终端将使用 IPython (替代标准Python终端)。 IPython 终端与其他相比更为强大，提供智能的自动补全，高亮输出，及其他特性。

* 安装 ipython

```bash
pip3 install ipython
```

* 如何使用

```bash
scrapy shell 域名

# 然后可在终端进行调试，例如
In [4]: response.xpath('//input[@id="info"]')
```



## 单管道使用

### yield 
* 简单理解就是 return 一个返回值，提交给 pipeline。并且记住这个返回位置，下次迭代就从这个位置后（下一行）开始


### 流程

* Step 1：数据定义 `items.py`

```python
class ScrapyItem(scrapy.Item):
    # 你要下载的数据都有什么
    # 图片
    src = scrapy.Field()

    # 名字
    name = scrapy.Field()

    # 价格
    price = scrapy.Field()
```

* Step 2：spider 获取数据

```python
# 这里导入 ScrapyItem 虽然飘红但是其实没有问题
from scrapy_.items import ScrapyItem

    def parse(self, response):
        print('======================================')

        # pipelines 下载数据的

        # items 定义数据结构的
        # //ul[@id="xxx"]/li//img/@src
        # //ul[@id="xxx"]/li//img/@alt
        # //ul[@id="xxx"]/li//span[@class="price"]/text()
        # Tips: 所有的 Selector 对象，都可以再次调用 xpath 方法
        li_list = response.xpath('//ul[@id="xxx"]/li')

        for li in li_list:
            # 在 li Selector 基础上再次调用 xpath 获取当前标签下的 img
            name = li.xpath('.//img/@alt').extract_first()
            price = li.xpath('.//span[@class="price"]/text()').extract_first()

            # 图片的坑
            # 1 这里注意图片有反爬懒加载，属性应该是 data-original，而不是 src
            # 2 第一张图片和其他图片的标签属性是不一样的
            # 即第一张图片的 src 是可以使用的
            # 其他图片的地址是 data-original
            src = li.xpath('.//img/@data-original').extract_first()
            if src is None:
                src = li.xpath('.//img/@src').extract_first()

            print(src, name, price)
            data = ScrapyItem(src=src, name=name, price=price)

            # 将对象给管道：获取一个 data 就将其交给 pipelines
            yield data
```


* Step 3：管道使用
1. 如果想使用管道的话，就必须在 `settings.py` 里开启管道，将其放开注释
2. 管道可以有很多个，管道是有优先级的，值的范围是 `1~1000`，值越小优先级越高

```python
ITEM_PIPELINES = {
   "scrapy.pipelines.ScrapyPipeline": 300,
}
```


* Step 4：spider 代码使用 yield 提交给管道 `pipelines.py`

```python
class Scrapy_Pipeline:
    # 在爬虫开始执行前 就执行的方法
    def open_spider(self, spider):
        print('++++++++++++++++++++++++')
        self.fp = open('data.json', 'w', encoding='utf-8')


    # item 就是 yield 后边返回的对象
    def process_item(self, item, spider):
        # 1 write 方法必须接收字符串，而不能是其他的对象
        # 解决：需要强转为 str(item)
        # 2 w 模式会每一个对象都打开一次文件，覆盖之前的内容然后关闭，
        # 解决：所以要用 a 模式（追加）
        # 这种写法不推荐，因为会频繁操作文件
        # with open('data.json', 'a', encoding='utf-8') as fp:
        #     fp.write(str(item))

        # 优化为这样
        self.fp.write(str(item))
        return item


    # 在爬虫文件执行结束后 执行的方法
    def close_spider(self, spider):
        print('--------------------------')
        self.fp.close()
```




## 多条管道下载

* Step 1：在 `pipelines.py` 定义添加新的管道类
* Step 2：在 `setting.py` 中开启管道: `"scrapy.pipelines.ScrapyDownloadPipeline": 301`，由于下载比较慢设置优先级低一些



```python
# 多条管道同时开启
# 1 定义新的管道类
# 2 在 setting.py 中开启管道: "scrapy.pipelines.ScrapyDownloadPipeline": 301,


import urllib.request
class ScrapyDownloadPipeline:

    def process_item(self, item, spider):
        url = 'http:' + item.get('src')
        filename = './images/' + item.get('name') + '.jpg'
        urllib.request.urlretrieve(url=url, filename=filename)

        return item
```



## 多页数据下载

* 多页下载必须调整 `allowed_domains` 的范围，一般只写域名


```python
import scrapy

# 这里导入 虽然飘红但是没有问题
from scrapy.items import ScrapyItem

class Spider(scrapy.Spider):
    name = ""
    # 如果是多页下载，必须调整 allowed_domains 的范围，一般只写域名
    allowed_domains = ["www.xxx.com"]
    start_urls = ["https://"]

    base_url = 'http://www.com/'
    page = 1
    def parse(self, response):
        print('======================================')

        # pipelines 下载数据的

        # items 定义数据结构的
        # //ul[@id="xxx"]/li//img/@src
        # //ul[@id="xxx"]/li//img/@alt
        # //ul[@id="xxx"]/li//span[@class="price"]/text()
        # 所有的 Selector 对象，都可以再次调用 xpath 方法
        li_list = response.xpath('//ul[@id="xxx"]/li')

        for li in li_list:
            # 在 li Selector 基础上再次调用 xpath 获取当前标签下的 img
            name = li.xpath('.//img/@alt').extract_first()
            price = li.xpath('.//span[@class="price"]/text()').extract_first()

            # 图片的坑
            # 1 这里注意图片有反爬懒加载，属性应该是 data-original，而不是 src
            # 2 第一张图片和其他图片的标签属性是不一样的
            # 即第一张图片的 src 是可以使用的
            # 其他图片的地址是 data-original
            src = li.xpath('.//img/@data-original').extract_first()
            if src is None:
                src = li.xpath('.//img/@src').extract_first()

            print(src, name, price)
            book = ScrapyItem(src=src, name=name, price=price)

            # 将对象给管道：获取一个 book 就将其交给 pipelines
            yield book


        if self.page < 3:
            self.page = self.page + 1
            url = self.base_url + str(self.page) + '.html'

            # 如何调用 parse 方法呢?
            # scrapy.Request 就是 scrapy 的 get 请求
            # 参数 url 就是请求地址
            # 参数 callback 就是要执行的函数，注意不能加 '()'
            yield scrapy.Request(url=url, callback=self.parse)
```



